{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "362fb904-0b70-40ca-a221-3c49d93d75bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Idea: Studying the Lok Sabha debates to identify trends in Indian politics since 1952: \n",
    "# I thought of using NLP methods to make use of the minehouse of information available on \n",
    "# the Indian Parliament's website to summarise key issues and answer questions that I am curious \n",
    "# about: How are the topics discussed during the debated different across ruling parties?, \n",
    "# What are the trends in the themes discussed since 1952? \n",
    "# Are topics that have gained most importance (measured as count of mentions) in discourses \n",
    "# also the ones which have seen the most public investment?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a9deb43-515d-47c5-94e6-5208b008fef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6faf271e-02da-4daf-a961-ff8e83c11585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_pdf_url(page_id):\n",
    "    url = 'https://eparlib.nic.in' + page_id\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    view_links = soup.find_all('a', string='View/Open')\n",
    "    pdf_url = view_links[0].get('href')\n",
    "    return pdf_url\n",
    "\n",
    "def download_file(url, output_folder):\n",
    "    response = requests.get(url)\n",
    "    filename = os.path.join(output_folder, url.split(\"/\")[-1])\n",
    "    with open(filename, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "output_path = r'C:\\Users\\vishe\\Desktop\\Indian Parliament'\n",
    "\n",
    "for i in range(326):\n",
    "    offset = i * 20\n",
    "    url = f'https://eparlib.nic.in/handle/123456789/7?offset={offset}'\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    view_links = soup.find_all('a', string='View...')\n",
    "    extracted_links = [link.get('href') for link in view_links]\n",
    "\n",
    "    for link in extracted_links:\n",
    "        pdf_url = 'https://eparlib.nic.in' + extract_pdf_url(link)\n",
    "        if pdf_url:\n",
    "            download_file(pdf_url, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd386178-bf39-4edd-9aef-5e086126eef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false\n",
    "\n",
    "# URL of the page to scrape\n",
    "url = \"https://eparlib.nic.in/handle/123456789/7\"\n",
    "\n",
    "# Send a GET request to the page\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all links with the anchor text 'View...'\n",
    "view_links = soup.find_all('a', string='View...')\n",
    "\n",
    "# Extract the href attributes from these links\n",
    "extracted_links = [link.get('href') for link in view_links]\n",
    "\n",
    "\n",
    "def extract_pdf_url(page_id):\n",
    "    url = 'https://eparlib.nic.in' + page_id\n",
    "    # print(f\"Querying: {url}\")\n",
    "    # Send a GET request to the page\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all links with the anchor text 'View/Open'\n",
    "    view_links = soup.find_all('a', string='View/Open')\n",
    "\n",
    "    pdf_url = view_links[0].get('href')\n",
    "    # print(f\"Got pdf: {pdf_url}\")\n",
    "    return pdf_url\n",
    "\n",
    "\n",
    "for i in range(326):\n",
    "    offset = i * 20\n",
    "    url = f'https://eparlib.nic.in/handle/123456789/7?offset={offset}'\n",
    "    \n",
    "    # Send a GET request to the page\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all links with the anchor text 'View...'\n",
    "    view_links = soup.find_all('a', string='View...')\n",
    "\n",
    "    # Extract the href attributes from these links\n",
    "    extracted_links = [link.get('href') for link in view_links]\n",
    "\n",
    "    print(extracted_links)\n",
    "\n",
    "def download_file(url, output_folder):\n",
    "    response = requests.get(url)\n",
    "    filename = os.path.join(output_folder, url.split(\"/\")[-1])\n",
    "    \n",
    "    with open(filename, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "   # Iterate through PDF links on each page and download\n",
    "for link in extracted_links:\n",
    "        pdf_url = 'https://eparlib.nic.in' + extract_pdf_url(link)\n",
    "        output_path=r'C:\\Users\\vishe\\Desktop\\Indian Parliament'\n",
    "        if pdf_url:\n",
    "            download_file(pdf_url,output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3555249e-9255-49fd-8125-ef78ffb6fd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name: lsd_01_01_13-05-1952\n",
      "File Name: lsd_01_01_15-05-1952\n",
      "File Name: lsd_01_01_16-05-1952\n",
      "File Name: lsd_17_12_10-08-2023\n",
      "File Name: lsd_17_12_11-08-2023\n",
      "File Name: lsd_17_13_18-09-2023\n",
      "File Name: lsd_17_13_19-09-2023\n",
      "File Name: lsd_17_13_20-09-2023\n",
      "File Name: lsd_17_13_21-09-2023\n",
      "File Name: lsd_17_14_04-12-2023\n",
      "File Name: lsd_17_14_05-12-2023\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>file_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lsd_01_01_13-05-1952</td>\n",
       "      <td>Ptt. S^.IX62\\n— m —\\nVdnme I  \\nW S K S  \\nTue...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              file_name                                       file_content\n",
       "0  lsd_01_01_13-05-1952  Ptt. S^.IX62\\n— m —\\nVdnme I  \\nW S K S  \\nTue..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import fitz\n",
    "import pandas as pd\n",
    "\n",
    "#pdf_folder = r\"C:\\Users\\vishe\\Desktop\\Indian Parliament\"\n",
    "pdf_folder = r\"C:\\Users\\vishe\\Desktop\\Indian Parliament\\practice\"\n",
    "\n",
    "# Lists to accumulate values\n",
    "file_names = []\n",
    "file_contents = []\n",
    "\n",
    "# Iterate over each PDF file in the folder\n",
    "for filename in os.listdir(pdf_folder):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(pdf_folder, filename)\n",
    "\n",
    "        # Use PyMuPDF to open and read the PDF\n",
    "        with fitz.open(pdf_path) as pdf_doc:\n",
    "            # Create a variable with the file name\n",
    "            variable_name = os.path.splitext(filename)[0]  # Removing the .pdf extension\n",
    "            variable_content = \"\"\n",
    "\n",
    "            # Iterate through each page in the PDF\n",
    "            for page_num in range(pdf_doc.page_count):\n",
    "                page = pdf_doc[page_num]\n",
    "                variable_content += page.get_text()\n",
    "\n",
    "            # Append values to the lists\n",
    "            file_names.append(variable_name)\n",
    "            file_contents.append(variable_content)\n",
    "\n",
    "            # Print for demonstration purposes\n",
    "            print(f\"File Name: {variable_name}\")\n",
    "\n",
    "# Create a DataFrame from the lists\n",
    "df = pd.DataFrame({\"file_name\": file_names, \"file_content\": file_contents})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(r'C:\\Users\\vishe\\Desktop\\Indian Parliament\\output_file.csv', index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc8dd3b8-8aea-4c4c-b807-cf18857f3c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false\n",
    "\n",
    "import os\n",
    "import fitz\n",
    "\n",
    "#pdf_folder = r\"C:\\Users\\vishe\\Desktop\\Indian Parliament\"\n",
    "pdf_folder=r\"C:\\Users\\vishe\\Desktop\\Indian Parliament\\practice\"\n",
    "\n",
    "# Iterate over each PDF file in the folder\n",
    "for filename in os.listdir(pdf_folder):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(pdf_folder, filename)\n",
    "\n",
    "        # Use PyMuPDF to open and read the PDF\n",
    "        with fitz.open(pdf_path) as pdf_doc:\n",
    "            # Create a variable with the file name\n",
    "            variable_name = os.path.splitext(filename)[0]  # Removing the .pdf extension\n",
    "            variable_content = \"\"\n",
    "\n",
    "            # Iterate through each page in the PDF\n",
    "            for page_num in range(pdf_doc.page_count):\n",
    "                page = pdf_doc[page_num]\n",
    "                variable_content += page.get_text()\n",
    "\n",
    "            # Now, you can use the variable_name and variable_content as needed\n",
    "            print(f\"File Name: {variable_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5240bdab-53f3-4de9-a35a-40b8a4aaeda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no: 11\n"
     ]
    }
   ],
   "source": [
    "# Count unique values in the 'file_name' column\n",
    "count = df['file_name'].nunique()\n",
    "\n",
    "print(\"no:\", count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "079d5a88-1cb5-4d29-b925-39b4f1b8370f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>file_content</th>\n",
       "      <th>content_eng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lsd_01_01_13-05-1952</td>\n",
       "      <td>Ptt. S^.IX62\\n— m —\\nVdnme I  \\nW S K S  \\nTue...</td>\n",
       "      <td>Ptt SIX62 m Vdnme I  W S K S  TuesdayNo 1   i ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              file_name                                       file_content  \\\n",
       "0  lsd_01_01_13-05-1952  Ptt. S^.IX62\\n— m —\\nVdnme I  \\nW S K S  \\nTue...   \n",
       "\n",
       "                                         content_eng  \n",
       "0  Ptt SIX62 m Vdnme I  W S K S  TuesdayNo 1   i ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Assuming df is your DataFrame with 'file_content' column\n",
    "df['content_eng'] = df['file_content'].apply(lambda x: re.sub(r'[^A-Za-z0-9 ]+', '', str(x)))\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d20e2faa-d123-4688-bfb6-4578e07e6921",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vishe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [(ReservedSch, 54), (CastesShri, 51), (Ram, 35...\n",
       "1    [(conventions, 23), (Parliament, 21), (Members...\n",
       "2    [(Members, 18), (introduce, 15), (motion, 15),...\n",
       "3    [(Telangana, 41), (English, 39), (crore, 35), ...\n",
       "4    [(Gazette, 11), (Act, 10), (Section, 9), (Augu...\n",
       "Name: top_words, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk import FreqDist\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# Get stopwords from NLTK\n",
    "nltk_stopwords_set = set(nltk_stopwords.words('english'))\n",
    "\n",
    "banned_words = ['sir', 'hon', 'shri', 'per', 'cent', 'government', 'india', 'will', 'central', 'people','question','papers', 'one','laid','prime','see','hrs', 'minister','would', 'also', 'dated', 'singh', 'dr', 'shrimati', 'interruptions', 'bill', 'state', 'women', 'kumar', 'house', 'country', 'speaker', 'district', 'sabha', 'report', 'library', 'tamil', 'distt', 'report', 'cum', 'lok', 'sabha', 'manipur', 'placed', 'want', 'regarding']\n",
    "\n",
    "# Combine spaCy's English stop words with your custom list\n",
    "stopwords_all = nltk_stopwords_set.union(banned_words)\n",
    "\n",
    "df['text_eng'] = df['content_eng'].apply(lambda x: re.sub(r'[^\\w\\s]', '', str(x)))\n",
    "df['text_eng'] = df['text_eng'].apply(lambda x: re.sub(r'[0-9]+', '', str(x)))\n",
    "\n",
    "df['tokens'] = df['text_eng'].apply(lambda x: str(x).split())\n",
    "\n",
    "# Use a list comprehension to flatten the list of lists\n",
    "df['filtered_tokens'] = df['tokens'].apply(lambda tokens: [word for word in tokens if (len(word) > 2) and (word.lower() not in stopwords_all)])\n",
    "\n",
    "# Create a new column 'top_words' containing a list of top words for each observation\n",
    "df['top_words'] = df['filtered_tokens'].apply(lambda tokens: FreqDist(tokens).most_common(10))  \n",
    "\n",
    "# Display the top words\n",
    "df['top_words'].head(5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f968228-753e-4534-8843-7ed59dfce736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.006*\"womens\" + 0.004*\"verma\" + 0.004*\"sunil\" + 0.004*\"ajay\" + 0.003*\"census\" + 0.003*\"choudhary\" + 0.003*\"reserved\" + 0.003*\"woman\" + 0.003*\"rajiv\" + 0.003*\"empowerment\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.038*\"space\" + 0.028*\"chandrayaan\" + 0.022*\"isro\" + 0.021*\"science\" + 0.008*\"launch\" + 0.007*\"madam\" + 0.007*\"proud\" + 0.006*\"landing\" + 0.005*\"successful\" + 0.005*\"achievements\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.006*\"railway\" + 0.005*\"legal\" + 0.004*\"telangana\" + 0.004*\"kerala\" + 0.003*\"coastal\" + 0.003*\"elections\" + 0.003*\"region\" + 0.003*\"funds\" + 0.003*\"process\" + 0.003*\"aiims\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "# Additional preprocessing steps\n",
    "#df['cleaned_content'] = df['file_content'].apply(lambda x: re.sub(r'[^\\w\\s]', '', str(x)))\n",
    "#df['cleaned_content'] = df['cleaned_content'].apply(lambda x: re.sub(r'[0-9]+', '', str(x)))\n",
    "\n",
    "# Tokenize and preprocess the documents\n",
    "processed_docs = [simple_preprocess(doc) for doc in df['text_eng']]\n",
    "\n",
    "# Create dictionary representation of the documents\n",
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "\n",
    "# Filter out tokens that appear in less than 3 documents or more than 50% of documents\n",
    "dictionary.filter_extremes(no_below=3, no_above=0.5)\n",
    "\n",
    "# Create bag-of-words representation of the documents\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "# Train LDA model\n",
    "lda_model = LdaModel(bow_corpus, num_topics=3, id2word=dictionary, passes=10)\n",
    "\n",
    "# Print the topics\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f'Topic: {idx} \\nWords: {topic}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bda44a-a56b-469d-8776-7019ed7d2563",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "\n",
    "from os import path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from operator import itemgetter\n",
    " \n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    " \n",
    "d = r\"C:\\\\Users\\\\VisheshikaVijayBahet\\\\Pictures\\\\\"\n",
    " \n",
    "stopwords = set(STOPWORDS)\n",
    "  \n",
    "clean_circle_mask = np.array(Image.open(path.join(d, \"map.png\")))\n",
    " \n",
    "wc_rect = WordCloud(background_color=\"white\", max_words=500, width=3000,\n",
    "                    height=1500, stopwords=stopwords, min_font_size=2,\n",
    "                    contour_width=3, contour_color='black')\n",
    "wc_rect.generate(text_eng)\n",
    "wc_rect.to_file(path.join(d, \"wc-rectangle.png\"))\n",
    " \n",
    "wc = WordCloud(background_color=\"white\", max_words=1000, width=2000,\n",
    "               height=1000, mask=clean_circle_mask, stopwords=stopwords,\n",
    "               min_font_size=2, contour_width=3, contour_color='black')\n",
    " \n",
    "wc.generate(text_eng)\n",
    "wc.to_file(path.join(d, \"wc-masked.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cc38a6-922f-4edf-95c5-0f27c31efd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from nltk import FreqDist\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import re\n",
    "\n",
    "banned_words= ['sir', 'hon', 'shri', 'per' ,'cent', 'government', 'india', 'will', 'central','people','one','minister']\n",
    "stopwords = set(STOPWORDS) \n",
    "stopwords_new = stopwords.union(banned_words)\n",
    "\n",
    "text_eng = re.sub(r'[^\\w\\s]', '', text_eng.lower())\n",
    "text_eng= re.sub(r'[0-9]+', '', text_eng)\n",
    "\n",
    "tokens = text_eng.split()\n",
    "\n",
    "\n",
    "filtered_tokens = [word for word in tokens if word not in stopwords_new]\n",
    "\n",
    "\n",
    "freq_dist = FreqDist(filtered_tokens)\n",
    "\n",
    "top_words = freq_dist.most_common(10)\n",
    "\n",
    "# Create a WordCloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(top_words))\n",
    "\n",
    "# Plot the WordCloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Print the count of the top 10 words\n",
    "print(\"Top 10 words and their counts:\")\n",
    "for word, count in top_words:\n",
    "    print(f\"{word}: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0b3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
